{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Speech Emotion Recognition - Multi-Dataset Training\n",
    "\n",
    "This notebook trains emotion recognition models on **multiple datasets** for improved accuracy:\n",
    "- **RAVDESS** - Ryerson Audio-Visual Database (1,440 files)\n",
    "- **CREMA-D** - Crowd-sourced Emotional Multimodal Actors Dataset (7,442 files)\n",
    "- **TESS** - Toronto Emotional Speech Set (2,800 files)\n",
    "- **SAVEE** - Surrey Audio-Visual Expressed Emotion (480 files)\n",
    "\n",
    "**Total: ~12,000+ audio samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q librosa soundfile kaggle tensorflow scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Datasets from Kaggle\n",
    "\n",
    "Enter your Kaggle credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Kaggle credentials\n",
    "KAGGLE_USERNAME = \"your_username\"  # @param {type:\"string\"}\n",
    "KAGGLE_KEY = \"your_api_key\"  # @param {type:\"string\"}\n",
    "\n",
    "# Setup Kaggle\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    f.write(f'{{\"username\":\"{KAGGLE_USERNAME}\",\"key\":\"{KAGGLE_KEY}\"}}')\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "print(\"‚úì Kaggle configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "!mkdir -p datasets\n",
    "\n",
    "# RAVDESS\n",
    "print(\"Downloading RAVDESS...\")\n",
    "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p datasets/ravdess --unzip -q\n",
    "\n",
    "# CREMA-D\n",
    "print(\"Downloading CREMA-D...\")\n",
    "!kaggle datasets download -d ejlok1/cremad -p datasets/cremad --unzip -q\n",
    "\n",
    "# TESS\n",
    "print(\"Downloading TESS...\")\n",
    "!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess -p datasets/tess --unzip -q\n",
    "\n",
    "# SAVEE\n",
    "print(\"Downloading SAVEE...\")\n",
    "!kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee -p datasets/savee --unzip -q\n",
    "\n",
    "print(\"\\n‚úì All datasets downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio configuration\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 3  # seconds\n",
    "N_MELS = 128\n",
    "N_MFCC = 40\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "# Unified emotion labels\n",
    "EMOTIONS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "def extract_mel_spectrogram(file_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"Extract mel spectrogram from audio file.\"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Pad or trim to fixed length\n",
    "        target_length = sr * duration\n",
    "        if len(y) < target_length:\n",
    "            y = np.pad(y, (0, target_length - len(y)))\n",
    "        else:\n",
    "            y = y[:target_length]\n",
    "        \n",
    "        # Extract mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH\n",
    "        )\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_features(file_path, sr=SAMPLE_RATE, duration=DURATION):\n",
    "    \"\"\"Extract combined features (MFCC, Chroma, Mel, Contrast, Tonnetz).\"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        \n",
    "        # Pad or trim\n",
    "        target_length = sr * duration\n",
    "        if len(y) < target_length:\n",
    "            y = np.pad(y, (0, target_length - len(y)))\n",
    "        else:\n",
    "            y = y[:target_length]\n",
    "        \n",
    "        # Extract features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        \n",
    "        # Aggregate features\n",
    "        features = np.hstack([\n",
    "            np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
    "            np.mean(chroma, axis=1), np.std(chroma, axis=1),\n",
    "            np.mean(mel, axis=1), np.std(mel, axis=1),\n",
    "            np.mean(contrast, axis=1), np.std(contrast, axis=1),\n",
    "            np.mean(tonnetz, axis=1), np.std(tonnetz, axis=1)\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ravdess(base_path):\n",
    "    \"\"\"Parse RAVDESS dataset.\"\"\"\n",
    "    # RAVDESS emotion mapping: 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised\n",
    "    emotion_map = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "        '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(base_path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                parts = f.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    emotion_code = parts[2]\n",
    "                    if emotion_code in emotion_map:\n",
    "                        files.append({\n",
    "                            'path': os.path.join(root, f),\n",
    "                            'emotion': emotion_map[emotion_code],\n",
    "                            'dataset': 'RAVDESS'\n",
    "                        })\n",
    "    return files\n",
    "\n",
    "def parse_cremad(base_path):\n",
    "    \"\"\"Parse CREMA-D dataset.\"\"\"\n",
    "    # CREMA-D emotion mapping\n",
    "    emotion_map = {\n",
    "        'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful',\n",
    "        'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'\n",
    "    }\n",
    "    \n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(base_path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                parts = f.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    emotion_code = parts[2]\n",
    "                    if emotion_code in emotion_map:\n",
    "                        files.append({\n",
    "                            'path': os.path.join(root, f),\n",
    "                            'emotion': emotion_map[emotion_code],\n",
    "                            'dataset': 'CREMA-D'\n",
    "                        })\n",
    "    return files\n",
    "\n",
    "def parse_tess(base_path):\n",
    "    \"\"\"Parse TESS dataset.\"\"\"\n",
    "    emotion_map = {\n",
    "        'angry': 'angry', 'disgust': 'disgust', 'fear': 'fearful',\n",
    "        'happy': 'happy', 'neutral': 'neutral', 'ps': 'surprised', 'sad': 'sad'\n",
    "    }\n",
    "    \n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(base_path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                # TESS format: OAF_word_emotion.wav or YAF_word_emotion.wav\n",
    "                parts = f.replace('.wav', '').split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    emotion = parts[-1].lower()\n",
    "                    if emotion in emotion_map:\n",
    "                        files.append({\n",
    "                            'path': os.path.join(root, f),\n",
    "                            'emotion': emotion_map[emotion],\n",
    "                            'dataset': 'TESS'\n",
    "                        })\n",
    "    return files\n",
    "\n",
    "def parse_savee(base_path):\n",
    "    \"\"\"Parse SAVEE dataset.\"\"\"\n",
    "    emotion_map = {\n",
    "        'a': 'angry', 'd': 'disgust', 'f': 'fearful',\n",
    "        'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprised'\n",
    "    }\n",
    "    \n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(base_path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                # SAVEE format: DC_a01.wav (emotion code at start after speaker)\n",
    "                name = f.replace('.wav', '')\n",
    "                for code, emotion in emotion_map.items():\n",
    "                    if f'_{code}' in name.lower() or name.lower().startswith(code):\n",
    "                        files.append({\n",
    "                            'path': os.path.join(root, f),\n",
    "                            'emotion': emotion,\n",
    "                            'dataset': 'SAVEE'\n",
    "                        })\n",
    "                        break\n",
    "    return files\n",
    "\n",
    "print(\"‚úì Dataset parsers defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Combine All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all datasets\n",
    "all_files = []\n",
    "\n",
    "# RAVDESS\n",
    "ravdess_path = 'datasets/ravdess'\n",
    "if os.path.exists(ravdess_path):\n",
    "    ravdess_files = parse_ravdess(ravdess_path)\n",
    "    all_files.extend(ravdess_files)\n",
    "    print(f\"RAVDESS: {len(ravdess_files)} files\")\n",
    "\n",
    "# CREMA-D\n",
    "cremad_path = 'datasets/cremad'\n",
    "if os.path.exists(cremad_path):\n",
    "    cremad_files = parse_cremad(cremad_path)\n",
    "    all_files.extend(cremad_files)\n",
    "    print(f\"CREMA-D: {len(cremad_files)} files\")\n",
    "\n",
    "# TESS\n",
    "tess_path = 'datasets/tess'\n",
    "if os.path.exists(tess_path):\n",
    "    tess_files = parse_tess(tess_path)\n",
    "    all_files.extend(tess_files)\n",
    "    print(f\"TESS: {len(tess_files)} files\")\n",
    "\n",
    "# SAVEE\n",
    "savee_path = 'datasets/savee'\n",
    "if os.path.exists(savee_path):\n",
    "    savee_files = parse_savee(savee_path)\n",
    "    all_files.extend(savee_files)\n",
    "    print(f\"SAVEE: {len(savee_files)} files\")\n",
    "\n",
    "print(f\"\\n‚úì Total files: {len(all_files)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_files)\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# By emotion\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "colors = ['#8e8e93', '#5ac8fa', '#ffcc00', '#5856d6', '#ff3b30', '#af52de', '#34c759', '#ff9500']\n",
    "axes[0].bar(emotion_counts.index, emotion_counts.values, color=colors[:len(emotion_counts)])\n",
    "axes[0].set_title('Samples by Emotion', fontsize=14)\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# By dataset\n",
    "dataset_counts = df['dataset'].value_counts()\n",
    "axes[1].pie(dataset_counts.values, labels=dataset_counts.index, autopct='%1.1f%%', \n",
    "            colors=['#0a84ff', '#30d158', '#ff9f0a', '#ff3b30'])\n",
    "axes[1].set_title('Samples by Dataset', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dataset_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Features from All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Extract mel spectrograms\n",
    "print(\"Extracting mel spectrograms...\")\n",
    "mel_features = []\n",
    "mel_labels = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    mel = extract_mel_spectrogram(row['path'])\n",
    "    if mel is not None:\n",
    "        mel_features.append(mel)\n",
    "        mel_labels.append(row['emotion'])\n",
    "\n",
    "X_mel = np.array(mel_features)\n",
    "y_mel = np.array(mel_labels)\n",
    "\n",
    "print(f\"\\n‚úì Mel spectrograms: {X_mel.shape}\")\n",
    "print(f\"‚úì Labels: {y_mel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract combined features for LSTM\n",
    "print(\"Extracting combined features...\")\n",
    "combined_features = []\n",
    "combined_labels = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    feat = extract_features(row['path'])\n",
    "    if feat is not None:\n",
    "        combined_features.append(feat)\n",
    "        combined_labels.append(row['emotion'])\n",
    "\n",
    "X_combined = np.array(combined_features)\n",
    "y_combined = np.array(combined_labels)\n",
    "\n",
    "print(f\"\\n‚úì Combined features: {X_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(EMOTIONS)\n",
    "\n",
    "y_mel_encoded = label_encoder.transform(y_mel)\n",
    "y_combined_encoded = label_encoder.transform(y_combined)\n",
    "\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Split data\n",
    "X_mel_train, X_mel_test, y_mel_train, y_mel_test = train_test_split(\n",
    "    X_mel, y_mel_encoded, test_size=0.2, random_state=42, stratify=y_mel_encoded\n",
    ")\n",
    "\n",
    "X_comb_train, X_comb_test, y_comb_train, y_comb_test = train_test_split(\n",
    "    X_combined, y_combined_encoded, test_size=0.2, random_state=42, stratify=y_combined_encoded\n",
    ")\n",
    "\n",
    "# Normalize combined features\n",
    "scaler = StandardScaler()\n",
    "X_comb_train_scaled = scaler.fit_transform(X_comb_train)\n",
    "X_comb_test_scaled = scaler.transform(X_comb_test)\n",
    "\n",
    "# Reshape for CNN (add channel dimension)\n",
    "X_mel_train = X_mel_train[..., np.newaxis]\n",
    "X_mel_test = X_mel_test[..., np.newaxis]\n",
    "\n",
    "print(f\"\\nTraining set: {X_mel_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_mel_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Enhanced Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization,\n",
    "    LSTM, Bidirectional, Input, Reshape, GlobalAveragePooling2D,\n",
    "    TimeDistributed, Attention, MultiHeadAttention, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "num_classes = len(EMOTIONS)\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    \"\"\"Enhanced CNN model with residual connections.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 4\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_cnn_lstm_model(input_shape):\n",
    "    \"\"\"Enhanced CNN-LSTM hybrid model.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN feature extraction\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    # Reshape for LSTM\n",
    "    shape = x.shape\n",
    "    x = Reshape((shape[1], shape[2] * shape[3]))(x)\n",
    "    \n",
    "    # Bidirectional LSTM\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"Enhanced LSTM model for combined features.\"\"\"\n",
    "    model = Sequential([\n",
    "        Reshape((1, input_shape[0]), input_shape=input_shape),\n",
    "        \n",
    "        Bidirectional(LSTM(256, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Model architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model\n",
    "print(\"=\"*60)\n",
    "print(\"Training CNN Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cnn_model = build_cnn_model(X_mel_train.shape[1:])\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    X_mel_train, y_mel_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_score = cnn_model.evaluate(X_mel_test, y_mel_test)\n",
    "print(f\"\\nCNN Test Accuracy: {cnn_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN-LSTM Model\n",
    "print(\"=\"*60)\n",
    "print(\"Training CNN-LSTM Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cnn_lstm_model = build_cnn_lstm_model(X_mel_train.shape[1:])\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "cnn_lstm_history = cnn_lstm_model.fit(\n",
    "    X_mel_train, y_mel_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_lstm_score = cnn_lstm_model.evaluate(X_mel_test, y_mel_test)\n",
    "print(f\"\\nCNN-LSTM Test Accuracy: {cnn_lstm_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM Model\n",
    "print(\"=\"*60)\n",
    "print(\"Training LSTM Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstm_model = build_lstm_model((X_comb_train_scaled.shape[1],))\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_comb_train_scaled, y_comb_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lstm_score = lstm_model.evaluate(X_comb_test_scaled, y_comb_test)\n",
    "print(f\"\\nLSTM Test Accuracy: {lstm_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate and Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history (all models)\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "models_data = [\n",
    "    ('CNN', cnn_history),\n",
    "    ('CNN-LSTM', cnn_lstm_history),\n",
    "    ('LSTM', lstm_history)\n",
    "]\n",
    "\n",
    "for i, (name, history) in enumerate(models_data):\n",
    "    # Accuracy\n",
    "    axes[0, i].plot(history.history['accuracy'], label='Train')\n",
    "    axes[0, i].plot(history.history['val_accuracy'], label='Val')\n",
    "    axes[0, i].set_title(f'{name} Accuracy')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].set_xlabel('Epoch')\n",
    "\n",
    "    # Loss\n",
    "    axes[1, i].plot(history.history['loss'], label='Train')\n",
    "    axes[1, i].plot(history.history['val_loss'], label='Val')\n",
    "    axes[1, i].set_title(f'{name} Loss')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].set_xlabel('Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save both names used in README (aliasing the same figure)\n",
    "plt.savefig('plots/all_models_training_history.png', dpi=200)\n",
    "plt.savefig('plots/training_history.png', dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print('‚úì Saved:', 'plots/all_models_training_history.png', 'plots/training_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices (all models)\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# CNN\n",
    "y_pred_cnn = np.argmax(cnn_model.predict(X_mel_test), axis=1)\n",
    "cm_cnn = confusion_matrix(y_mel_test, y_pred_cnn)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "axes[0].set_title(f'CNN (Acc: {cnn_score[1]*100:.1f}%)')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# CNN-LSTM\n",
    "y_pred_cnn_lstm = np.argmax(cnn_lstm_model.predict(X_mel_test), axis=1)\n",
    "cm_cnn_lstm = confusion_matrix(y_mel_test, y_pred_cnn_lstm)\n",
    "sns.heatmap(cm_cnn_lstm, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "axes[1].set_title(f'CNN-LSTM (Acc: {cnn_lstm_score[1]*100:.1f}%)')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "# LSTM\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(X_comb_test_scaled), axis=1)\n",
    "cm_lstm = confusion_matrix(y_comb_test, y_pred_lstm)\n",
    "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Oranges', ax=axes[2],\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "axes[2].set_title(f'LSTM (Acc: {lstm_score[1]*100:.1f}%)')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save both names used in README (aliasing the same figure)\n",
    "plt.savefig('plots/all_models_confusion_matrices.png', dpi=200)\n",
    "plt.savefig('plots/confusion_matrix.png', dpi=200)\n",
    "plt.show()\n",
    "\n",
    "print('‚úì Saved:', 'plots/all_models_confusion_matrices.png', 'plots/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download plots (Colab)\n",
    "# This will zip the `plots/` folder and download it.\n",
    "import os\n",
    "\n",
    "if not os.path.exists('plots'):\n",
    "    raise FileNotFoundError(\"plots/ folder not found. Run the plot-generation cells first.\")\n",
    "\n",
    "!zip -r plots_multi_dataset.zip plots/ -q\n",
    "print('‚úì Created plots_multi_dataset.zip')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('plots_multi_dataset.zip')\n",
    "    print('‚úì Download started')\n",
    "except Exception as e:\n",
    "    print('Not running in Colab (or download not available).')\n",
    "    print('Zip file saved as: plots_multi_dataset.zip')\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CNN:      {cnn_score[1]*100:.2f}% accuracy\")\n",
    "print(f\"CNN-LSTM: {cnn_lstm_score[1]*100:.2f}% accuracy\")\n",
    "print(f\"LSTM:     {lstm_score[1]*100:.2f}% accuracy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Classification reports\n",
    "print(\"\\nCNN Classification Report:\")\n",
    "print(classification_report(y_mel_test, y_pred_cnn, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nCNN-LSTM Classification Report:\")\n",
    "print(classification_report(y_mel_test, y_pred_cnn_lstm, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nLSTM Classification Report:\")\n",
    "print(classification_report(y_comb_test, y_pred_lstm, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "!mkdir -p models\n",
    "\n",
    "# Save models\n",
    "cnn_model.save('models/emotion_model_cnn.keras')\n",
    "cnn_lstm_model.save('models/emotion_model_cnn_lstm.keras')\n",
    "lstm_model.save('models/emotion_model_lstm.keras')\n",
    "\n",
    "# Save label encoder and scaler\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "with open('models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"‚úì All models saved!\")\n",
    "print(\"\\nFiles:\")\n",
    "!ls -la models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip models for download\n",
    "!zip -r trained_models_multi_dataset.zip models/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('trained_models_multi_dataset.zip')\n",
    "\n",
    "print(\"\\n‚úì Download complete!\")\n",
    "print(\"\\nTo use these models:\")\n",
    "print(\"1. Extract the zip file\")\n",
    "print(\"2. Copy the 'models' folder to your SpeechEmotionRecognition project\")\n",
    "print(\"3. Run: python3 app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
