{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Speech Emotion Recognition - Optimized Architectures\n",
    "\n",
    "This notebook implements **state-of-the-art** model architectures for improved accuracy:\n",
    "\n",
    "1. **Attention-based CNN** - Self-attention mechanisms for better feature focus\n",
    "2. **Transformer Encoder** - Multi-head attention for temporal patterns\n",
    "3. **EfficientNet Transfer Learning** - Pre-trained features\n",
    "4. **Ensemble Model** - Combines multiple models for best accuracy\n",
    "\n",
    "**Optimizations:**\n",
    "- Data augmentation (time stretch, pitch shift, noise)\n",
    "- Mixup training\n",
    "- Label smoothing\n",
    "- Learning rate scheduling with warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa soundfile kaggle tensorflow scikit-learn matplotlib seaborn audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle credentials\n",
    "KAGGLE_USERNAME = \"your_username\"  # @param {type:\"string\"}\n",
    "KAGGLE_KEY = \"your_api_key\"  # @param {type:\"string\"}\n",
    "\n",
    "os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
    "    f.write(f'{{\"username\":\"{KAGGLE_USERNAME}\",\"key\":\"{KAGGLE_KEY}\"}}')\n",
    "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "# Download RAVDESS and CREMA-D\n",
    "!mkdir -p datasets\n",
    "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p datasets/ravdess --unzip -q\n",
    "!kaggle datasets download -d ejlok1/cremad -p datasets/cremad --unzip -q\n",
    "print(\"âœ“ Datasets downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import audiomentations as A\n",
    "\n",
    "# Audio augmentation pipeline\n",
    "augment = A.Compose([\n",
    "    A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    A.TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    A.PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    A.Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "])\n",
    "\n",
    "def augment_audio(y, sr):\n",
    "    \"\"\"Apply augmentation to audio.\"\"\"\n",
    "    return augment(samples=y, sample_rate=sr)\n",
    "\n",
    "print(\"âœ“ Augmentation pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 3\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "EMOTIONS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "def extract_mel_spectrogram(file_path, augment_data=False):\n",
    "    \"\"\"Extract mel spectrogram with optional augmentation.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "        \n",
    "        # Pad/trim\n",
    "        target_len = SAMPLE_RATE * DURATION\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)))\n",
    "        else:\n",
    "            y = y[:target_len]\n",
    "        \n",
    "        # Augment if requested\n",
    "        if augment_data:\n",
    "            y = augment_audio(y, sr)\n",
    "        \n",
    "        # Extract mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        \n",
    "        return mel_db\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def parse_ravdess(path):\n",
    "    emotion_map = {'01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
    "                   '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'}\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                parts = f.split('-')\n",
    "                if len(parts) >= 3 and parts[2] in emotion_map:\n",
    "                    files.append({'path': os.path.join(root, f), 'emotion': emotion_map[parts[2]]})\n",
    "    return files\n",
    "\n",
    "def parse_cremad(path):\n",
    "    emotion_map = {'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fearful',\n",
    "                   'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'}\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            if f.endswith('.wav'):\n",
    "                parts = f.split('_')\n",
    "                if len(parts) >= 3 and parts[2] in emotion_map:\n",
    "                    files.append({'path': os.path.join(root, f), 'emotion': emotion_map[parts[2]]})\n",
    "    return files\n",
    "\n",
    "# Load files\n",
    "all_files = parse_ravdess('datasets/ravdess') + parse_cremad('datasets/cremad')\n",
    "df = pd.DataFrame(all_files)\n",
    "print(f\"Total files: {len(df)}\")\n",
    "print(df['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features with augmentation\n",
    "print(\"Extracting features...\")\n",
    "X, y = [], []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # Original\n",
    "    mel = extract_mel_spectrogram(row['path'], augment_data=False)\n",
    "    if mel is not None:\n",
    "        X.append(mel)\n",
    "        y.append(row['emotion'])\n",
    "    \n",
    "    # Augmented (2x data)\n",
    "    mel_aug = extract_mel_spectrogram(row['path'], augment_data=True)\n",
    "    if mel_aug is not None:\n",
    "        X.append(mel_aug)\n",
    "        y.append(row['emotion'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(f\"\\nDataset shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_LR = 0.001\n",
    "\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = steps_per_epoch * 5\n",
    "\n",
    "lr_schedule = WarmupCosineDecay(INITIAL_LR, warmup_steps, total_steps)\n",
    "\n",
    "# Callbacks - NOTE: Don't use ReduceLROnPlateau with custom LR schedule\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# For models using fixed LR (not schedule), use these callbacks:\n",
    "callbacks_with_lr_reduce = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Attention CNN\n",
    "print(\"=\"*60)\n",
    "print(\"Training Attention CNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "attn_cnn = build_attention_cnn(X_train.shape[1:])\n",
    "attn_cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Use callbacks without ReduceLROnPlateau since we're using custom LR schedule\n",
    "attn_history = attn_cnn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,  # Only EarlyStopping, no ReduceLROnPlateau\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "attn_score = attn_cnn.evaluate(X_test, y_test)\n",
    "print(f\"\\nAttention CNN Accuracy: {attn_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer\n",
    "print(\"=\"*60)\n",
    "print(\"Training Transformer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "transformer = build_transformer_model(X_train.shape[1:])\n",
    "transformer.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Use callbacks with ReduceLROnPlateau since we're using fixed LR\n",
    "trans_history = transformer.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_with_lr_reduce,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "trans_score = transformer.evaluate(X_test, y_test)\n",
    "print(f\"\\nTransformer Accuracy: {trans_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EfficientNet\n",
    "print(\"=\"*60)\n",
    "print(\"Training EfficientNet\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "efficientnet = build_efficientnet_model(X_train.shape[1:])\n",
    "efficientnet.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Use callbacks with ReduceLROnPlateau since we're using fixed LR\n",
    "eff_history = efficientnet.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fewer epochs for transfer learning\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_with_lr_reduce,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "eff_score = efficientnet.evaluate(X_test, y_test)\n",
    "print(f\"\\nEfficientNet Accuracy: {eff_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_model(input_shape):\n",
    "    \"\"\"EfficientNet transfer learning model.\"\"\"\n",
    "    # Convert grayscale to RGB\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(3, 1, padding='same')(inputs)  # 1 channel to 3\n",
    "    \n",
    "    # Resize to EfficientNet input size\n",
    "    x = layers.Resizing(224, 224)(x)\n",
    "    \n",
    "    # Load EfficientNetB0\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=x\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add classification head\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ EfficientNet model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training with Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixup data augmentation\n",
    "def mixup(x, y, alpha=0.2):\n",
    "    \"\"\"Mixup training augmentation.\"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    indices = tf.random.shuffle(tf.range(batch_size))\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    x_mixed = lam * x + (1 - lam) * tf.gather(x, indices)\n",
    "    y_mixed = lam * y + (1 - lam) * tf.gather(y, indices)\n",
    "    \n",
    "    return x_mixed, y_mixed\n",
    "\n",
    "# Learning rate schedule with warmup\n",
    "class WarmupCosineDecay(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_lr = self.initial_lr * (step / self.warmup_steps)\n",
    "        decay_lr = self.initial_lr * 0.5 * (\n",
    "            1 + tf.cos(np.pi * (step - self.warmup_steps) / (self.total_steps - self.warmup_steps))\n",
    "        )\n",
    "        return tf.where(step < self.warmup_steps, warmup_lr, decay_lr)\n",
    "\n",
    "print(\"âœ“ Training utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_LR = 0.001\n",
    "\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "warmup_steps = steps_per_epoch * 5\n",
    "\n",
    "lr_schedule = WarmupCosineDecay(INITIAL_LR, warmup_steps, total_steps)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Attention CNN\n",
    "print(\"=\"*60)\n",
    "print(\"Training Attention CNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "attn_cnn = build_attention_cnn(X_train.shape[1:])\n",
    "attn_cnn.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "attn_history = attn_cnn.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "attn_score = attn_cnn.evaluate(X_test, y_test)\n",
    "print(f\"\\nAttention CNN Accuracy: {attn_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer\n",
    "print(\"=\"*60)\n",
    "print(\"Training Transformer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "transformer = build_transformer_model(X_train.shape[1:])\n",
    "transformer.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "trans_history = transformer.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "trans_score = transformer.evaluate(X_test, y_test)\n",
    "print(f\"\\nTransformer Accuracy: {trans_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EfficientNet\n",
    "print(\"=\"*60)\n",
    "print(\"Training EfficientNet\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "efficientnet = build_efficientnet_model(X_train.shape[1:])\n",
    "efficientnet.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "eff_history = efficientnet.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Fewer epochs for transfer learning\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "eff_score = efficientnet.evaluate(X_test, y_test)\n",
    "print(f\"\\nEfficientNet Accuracy: {eff_score[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble predictions\n",
    "def ensemble_predict(models, X, weights=None):\n",
    "    \"\"\"Weighted ensemble prediction.\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1/len(models)] * len(models)\n",
    "    \n",
    "    predictions = np.zeros((len(X), num_classes))\n",
    "    for model, weight in zip(models, weights):\n",
    "        predictions += weight * model.predict(X)\n",
    "    \n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate ensemble accuracy\n",
    "models = [attn_cnn, transformer, efficientnet]\n",
    "weights = [attn_score[1], trans_score[1], eff_score[1]]  # Weight by accuracy\n",
    "weights = [w / sum(weights) for w in weights]  # Normalize\n",
    "\n",
    "y_pred_ensemble = ensemble_predict(models, X_test, weights)\n",
    "ensemble_acc = np.mean(y_pred_ensemble == y_test)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Attention CNN:  {attn_score[1]*100:.2f}%\")\n",
    "print(f\"Transformer:    {trans_score[1]*100:.2f}%\")\n",
    "print(f\"EfficientNet:   {eff_score[1]*100:.2f}%\")\n",
    "print(f\"Ensemble:       {ensemble_acc*100:.2f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "best_model = attn_cnn if attn_score[1] >= max(trans_score[1], eff_score[1]) else \\\n",
    "             transformer if trans_score[1] >= eff_score[1] else efficientnet\n",
    "best_name = 'Attention CNN' if best_model == attn_cnn else \\\n",
    "            'Transformer' if best_model == transformer else 'EfficientNet'\n",
    "\n",
    "y_pred = np.argmax(best_model.predict(X_test), axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(f'{best_name} Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_optimized.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{best_name} Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models\n",
    "\n",
    "# Save all models\n",
    "attn_cnn.save('models/emotion_model_cnn.keras')  # Replace default CNN\n",
    "transformer.save('models/emotion_model_transformer.keras')\n",
    "efficientnet.save('models/emotion_model_efficientnet.keras')\n",
    "\n",
    "# Save label encoder\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"âœ“ Models saved!\")\n",
    "!ls -la models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "!zip -r optimized_models.zip models/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('optimized_models.zip')\n",
    "\n",
    "print(\"\\nâœ“ Download complete!\")\n",
    "print(\"\\nTo use: Extract and copy 'models' folder to your project.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
